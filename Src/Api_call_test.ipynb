{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from db_connector import DB\n",
    "from logger import logging\n",
    "\n",
    "Task_URL = \"https://api.apify.com/v2/actor-tasks/yellow_saint~instagram-scraper-task/input?token=apify_api_qIc4Lwctqfr2WAFjmyQZ2mwd1zYhwO0JFLvE\"\n",
    "Scrape_Link_dataset = \"https://api.apify.com/v2/actor-tasks/yellow_saint~instagram-scraper-task/run-sync-get-dataset-items?token=apify_api_qIc4Lwctqfr2WAFjmyQZ2mwd1zYhwO0JFLvE\"\n",
    "Insta_Link = 'https://www.instagram.com/{x}/'\n",
    "inputs  =  requests.get(Task_URL)\n",
    "\n",
    "\n",
    "\n",
    "#apify_client = ApifyClient(token)\n",
    "\n",
    "# Start an actor and wait for it to finish\n",
    "#task_call = apify_client.task('yellow_saint~instagram-scraper-task').call(task_input=obj_list[0])\n",
    "\n",
    "# Fetch results from the actor run's default dataset\n",
    "#dataset_items = apify_client.dataset(task_call['defaultDatasetId']).list_items().items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = inputs.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3265431808.py, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 72\u001b[1;36m\u001b[0m\n\u001b[1;33m    for i in\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Scrape():\n",
    "    Task_URL = \"https://api.apify.com/v2/actor-tasks/yellow_saint~instagram-scraper-task/input?token=apify_api_qIc4Lwctqfr2WAFjmyQZ2mwd1zYhwO0JFLvE\"\n",
    "    Scrape_Link_dataset = \"https://api.apify.com/v2/actor-tasks/yellow_saint~instagram-scraper-task/run-sync-get-dataset-items?token=apify_api_qIc4Lwctqfr2WAFjmyQZ2mwd1zYhwO0JFLvE\"\n",
    "    Insta_Link = 'https://www.instagram.com/{x}/'\n",
    "    inputs  =  requests.get(Task_URL)\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.inputs  =  requests.get(self.ask_URL)\n",
    "        self.DB = DB()\n",
    "        logging.info(\"DB connected at Scraper\")\n",
    "        self.To_scan()\n",
    "        \n",
    "    def To_scan(self) -> list:\n",
    "        Unsorted_list = self.DB.pull(db = \"Clean\",document=\"To_scan\",filter= {},colunm={\"_id\":0})\n",
    "        sorted_data = sorted(Unsorted_list, key=lambda x: x['priority'])\n",
    "        final_list = []\n",
    "        for i in sorted_data:\n",
    "            final_list.append(i[\"id\"])\n",
    "        self.To_scan = final_list    \n",
    "        logging.info(\"To_scan listed Created\")\n",
    "\n",
    "    \n",
    "    async def get(\n",
    "        session: aiohttp.ClientSession,\n",
    "        input_obj:dict,\n",
    "        url : str\n",
    "        ) -> dict:\n",
    "\n",
    "        print(f\"Requesting {url}\")\n",
    "        resp = await session.request('POST', url=url,json=input_obj)\n",
    "\n",
    "        # Note that this may raise an exception for non-2xx responses\n",
    "        # You can either handle that here, or pass the exception through\n",
    "        data = await resp.json()\n",
    "        print(f\"Received data for {url}\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    async def main(self,input_objs,url, **kwargs):\n",
    "        # Asynchronous context manager.  Prefer this rather\n",
    "        # than using a different session for each GET request\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for obj in input_objs:\n",
    "                tasks.append(self.get(session=session, url=url,input_obj=obj, **kwargs))\n",
    "            # asyncio.gather() will wait on the entire task set to be\n",
    "            # completed.  If you want to process results greedily as they come in,\n",
    "            # loop over asyncio.as_completed()\n",
    "            data = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            return data\n",
    "    def Cleaner(self,data) -> None:\n",
    "        \"\"\"\n",
    "            The Scrape function produces a data in the form of 2-d nested Lists -> list[list] \n",
    "            To pull the lists out of the main list we use 2 loops to pull them out\n",
    "            can be done with recrusion but will try later,\n",
    "            Also traverse the data to find more related profiles to further scan       \n",
    "        \"\"\"\n",
    "        cleaned_data = []\n",
    "        for i in data:\n",
    "            for j in i:\n",
    "                cleaned_data.append(j)\n",
    "        \n",
    "        for i in cleaned_data:\n",
    "\n",
    "            for j in i[\"relatedProfiles\"]:\n",
    "                to_scan_dict = {\"id\":j[\"username\"],\"priority\":2}\n",
    "                self.DB.push(to_scan_dict,\"Clean\",\"To_scan\")\n",
    "            i[\"Date\"] = datetime.date   \n",
    "            self.DB.push(i,\"Raw\",\"Creators\")        \n",
    "    \n",
    "\n",
    "    def input_obj_maker(List_of_url:list,No_of_acc:int) -> list:\n",
    "\n",
    "        obj_list = []\n",
    "        for i in  range(0,len(List_of_url)-No_of_acc,No_of_acc):\n",
    "            temp = copy.deepcopy(obj)\n",
    "            temp[\"directUrls\"] =  List_of_url[i:i+No_of_acc]\n",
    "            obj_list.append(temp)\n",
    "        return  obj_list  \n",
    "\n",
    "    async def scrape(self):\n",
    "        data = await self.main( self.To_scan,self.Scrape_Link_dataset)\n",
    "        \n",
    "        return self.Cleaner(data) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get(\n",
    "    session: aiohttp.ClientSession,\n",
    "    input_obj:dict,\n",
    "    url : str\n",
    ") -> dict:\n",
    "    print(f\"Requesting {url}\")\n",
    "    resp = await session.request('POST', url=url,json=input_obj)\n",
    "\n",
    "    # Note that this may raise an exception for non-2xx responses\n",
    "    # You can either handle that here, or pass the exception through\n",
    "    data = await resp.json()\n",
    "    print(f\"Received data for {url}\")\n",
    "    return data\n",
    "async def main(input_objs,url, **kwargs):\n",
    "    # Asynchronous context manager.  Prefer this rather\n",
    "    # than using a different session for each GET request\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for obj in input_objs:\n",
    "            tasks.append(get(session=session, url=url,input_obj=obj, **kwargs))\n",
    "        # asyncio.gather() will wait on the entire task set to be\n",
    "        # completed.  If you want to process results greedily as they come in,\n",
    "        # loop over asyncio.as_completed()\n",
    "        data = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[545, 54, 54545]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.To_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2},\n",
       " {'id': 545, 'priority': 1},\n",
       " {'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2},\n",
       " {'id': 545, 'priority': 1},\n",
       " {'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2},\n",
       " {'id': 545, 'priority': 1},\n",
       " {'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2},\n",
       " {'id': 545, 'priority': 1},\n",
       " {'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2},\n",
       " {'id': 545, 'priority': 1},\n",
       " {'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2},\n",
       " {'id': 545, 'priority': 1},\n",
       " {'id': 54545, 'priority': 3},\n",
       " {'id': 545, 'priority': 2}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = obj.To_scan\n",
    "for i in data:\n",
    "    data.append(i)\n",
    "    if len(data) == 20:\n",
    "        break\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 545, 'priority': 1}\n",
      "{'id': 545, 'priority': 1}\n",
      "{'id': 545, 'priority': 1}\n",
      "{'id': 545, 'priority': 1}\n",
      "{'id': 545, 'priority': 1}\n",
      "{'id': 545, 'priority': 1}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 545, 'priority': 2}\n",
      "{'id': 54545, 'priority': 3}\n",
      "{'id': 54545, 'priority': 3}\n",
      "{'id': 54545, 'priority': 3}\n",
      "{'id': 54545, 'priority': 3}\n",
      "{'id': 54545, 'priority': 3}\n",
      "{'id': 54545, 'priority': 3}\n",
      "{'id': 54545, 'priority': 3}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = []\n",
    "for i in data:\n",
    "    for j in i:\n",
    "        data_2.append(j)\n",
    "to_scan = []\n",
    "for i in data_2:\n",
    "    for j in i[\"relatedProfiles\"]:\n",
    "        to_scan.append(Insta_Link.format(x = j[\"username\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = input_obj_maker(to_scan,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_scan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Insta_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
